# 图解机器学习 | KNN 算法及其应用

> 原文：[`blog.csdn.net/ShowMeAI/article/details/123391033`](https://blog.csdn.net/ShowMeAI/article/details/123391033)

![在这里插入图片描述](img/4432f9dc4269ed9784c7f5a666c43254.png)

作者：[韩信子](https://github.com/HanXinzi-AI)@[ShowMeAI](http://www.showmeai.tech/)
[教程地址](http://www.showmeai.tech/tutorials/34)：[`www.showmeai.tech/tutorials/34`](http://www.showmeai.tech/tutorials/34)
[本文地址](http://www.showmeai.tech/article-detail/187)：[`www.showmeai.tech/article-detail/187`](http://www.showmeai.tech/article-detail/187)
**声明：版权所有，转载请联系平台与作者并注明出处**

* * *

# 引言

K 近邻算法（k-nearest neighbors，KNN，有些地方也译作「K 近邻算法」）是一种很基本朴实的机器学习方法。

KNN 在我们日常生活中也有类似的思想应用，比如，我们判断一个人的人品，往往只需要观察他最密切的几个人的人品好坏就能得到结果了。这就是 KNN 的思想应用，KNN 方法既可以做分类，也可以做回归。在本篇内容中，我们来给大家展开讲解 KNN 相关的知识原理。

（本篇 KNN 部分内容涉及到机器学习基础知识，没有先序知识储备的宝宝可以查看 ShowMeAI 的文章 [图解机器学习 | 机器学习基础知识](http://www.showmeai.tech/article-detail/185)。

# 1.机器学习与分类问题

## 1）分类问题

**分类问题是机器学习非常重要的一个组成部分，它的目标是根据已知样本的某些特征，判断一个样本属于哪个类别**。分类问题可以细分如下：

*   **二分类问题**：表示分类任务中有两个类别新的样本属于哪种已知的样本类。

*   **多类分类**（Multiclass classification）问题：表示分类任务中有多类别。

*   **多标签分类**（Multilabel classification）问题：给每个样本一系列的目标标签。

![](img/16dff78ef65ea8d1ca3a6b054778b0e5.png)

## 2）分类问题的数学抽象

**从算法的角度解决一个分类问题，我们的训练数据会被映射成 n 维空间的样本点（这里的 n 就是特征维度），我们需要做的事情是对 n 维样本空间的点进行类别区分，某些点会归属到某个类别**。

下图所示的是二维平面中的两类样本点，我们的模型（分类器）在学习一种区分不同类别的方法，比如这里是使用一条直线去对 2 类不同的样本点进行切分。

![](img/a700e998943c286de0319280319f2bb5.png)

常见的分类问题应用场景很多，我们选择几个进行举例说明：

*   **垃圾邮件识别**：可以作为二分类问题，将邮件分为你「垃圾邮件」或者「正常邮件」。

*   **图像内容识别**：因为图像的内容种类不止一个，图像内容可能是猫、狗、人等等，因此是多类分类问题。

*   **文本情感分析**：既可以作为二分类问题，将情感分为褒贬两种，还可以作为多类分类问题，将情感种类扩展，比如分为：十分消极、消极、积极、十分积极等。

# 2.K 近邻算法核心思想

在模式识别领域中，K 近邻算法（KNN 算法，又译 K-最近邻算法）是一种用于分类和回归的非参数统计方法。在这两种情况下，输入包含特征空间中的 K 个最接近的训练样本。

## 1）K 近邻核心思想

在 KNN 分类中，输出是一个分类族群。**一个对象的分类是由其邻居的「多数表决」确定的**，K 个最近邻居（K 为正整数，通常较小）中最常见的分类决定了赋予该对象的类别。

*   若 K=1，则该对象的类别直接由最近的一个节点赋予。

> 在 KNN 回归中，输出是该对象的属性值。该值是其 K 个最近邻居的值的平均值。

![](img/e0b74b1b79087c9ea671f9309138413a.png)

**K 近邻居法采用向量空间模型来分类，概念为相同类别的案例，彼此的相似度高**。而可以借由计算与已知类别案例之相似度，来评估未知类别案例可能的分类。

KNN 是一种基于实例的学习，或者是局部近似和将所有计算推迟到分类之后的惰性学习。K-近邻算法是所有的机器学习算法中最简单的之一。

## 2）豆子分类例子

想一想：下图中只有三种豆，有三个豆的种类未知，如何判定他们的种类？

![](img/c6886fb74d2362341e9c2b7046389c44.png)

1968 年，Cover 和 Hart 提出了最初的近邻法，思路是——未知的豆离哪种豆最近，就认为未知豆和该豆是同一种类。

![](img/6e8dbc1cb2364a0faf92e8b87f29e938.png)

由此，引出最近邻算法的定义：为了判定未知样本的类别，以全部训练样本作为代表点计算未知样本与所有训练样本的距离，并以最近邻者的类别作为决策未知样本类别的唯一依据。

**最近邻算法的缺陷是对噪声数据过于敏感**。从图中可以得到，一个圈起来的蓝点和两个圈起来的红点到绿点的距离是相等的，根据最近邻算法，该点的形状无法判断。

为了解决这个问题，我们可以把位置样本周边的多个最近样本计算在内，扩大参与决策的样本量，以避免个别数据直接决定决策结果。

![](img/1f1a9bf3e3bc71dd52dd9fb5ed78ecfe.png)

引进 K-近邻算法——选择未知样本一定范围内确定个数的 K 个样本，该 K 个样本大多数属于某一类型，则未知样本判定为该类型。K-近邻算法是最近邻算法的一个延伸。

> 根据 K 近邻算法，离绿点最近的三个点中有两个是红点，一个是蓝点，红点的样本数量多于蓝点的样本数量，因此绿点的类别被判定为红点。

# 3.K 近邻算法步骤与示例

下面的内容首先为大家梳理下 K 近邻算法的步骤，之后通过示例为大家展示 K 近邻算法的计算流程。

![](img/be199ed69d30f8d61cf07097d16f9543.png)

## 1）K 近邻算法工作原理

*   存在一个样本数据集合，也称作训练样本集，并且样本集中每个数据都存在标签，即我们知道样本集中每个数据与所属分类的对应关系。

*   输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本集中特征最相似数据（最近邻）的分类标签。

*   **一般来说，只选择样本数据集中前 N 个最相似的数据。K 一般不大于 20**，最后，选择 K 个中出现次数最多的分类，作为新数据的分类。

## 2）K 近邻算法参数选择

*   **如何选择一个最佳的 K 值取决于数据**。一般情况下，在分类时较大的 K 值能够减小噪声的影响， 但会使类别之间的界限变得模糊。一个较好的 K 值能通过各种启发式技术（见超参数优化）来获取。

*   噪声和非相关性特征的存在，或特征尺度与它们的重要性不一致会使 K 近邻算法的准确性严重降低。对于选取和缩放特征来改善分类已经做了很多研究。一个普遍的做法是**利用进化算法优化功能扩展**，还有一种较普遍的方法是**利用训练样本的互信息进行选择特征**。

*   在二元（两类）分类问题中，选取 K 为奇数有助于避免两个分类平票的情形。在此问题下，选取最佳经验 K 值的方法是自助法。

> 说明：KNN 没有显示的训练过程，它是「懒惰学习」的代表，它在训练阶段只是把数据保存下来，训练时间开销为 0，等收到测试样本后进行处理。

## 3）K 近邻算法示例

举例：以电影分类作为例子，电影题材可分为爱情片，动作片等。那么爱情片有哪些特征？动作片有哪些特征呢？也就是说给定一部电影，怎么进行分类？

这里假定将电影分为爱情片和动作片两类，如果一部电影中接吻镜头很多，打斗镜头较少，显然是属于爱情片，反之为动作片。

![](img/1faddadde500cde1972eff842026cee2.png)

有人曾根据电影中打斗动作和接吻动作数量进行评估，数据如图。给定一部电影数据（18,90）打斗镜头 18 个，接吻镜头 90 个，如何知道它是什么类型的呢？

现在我们按照距离的递增顺序排序，可以找到 k 个距离最近的电影。

![](img/24f772fef357963769ad77a70c987b0d.png)

假如 K=3，那么来看排序的前 3 个电影的类别，都是爱情片，根据 KNN 的投票机制，我们判定这部电影属于爱情片。（这里的 K 是超参数，可以调整，如果取 K=4，那可能投票的 4 部电影分别是 爱情片、爱情片、爱情片、动作片，但本例中判定结果依旧为爱情片）

# 4.K 近邻算法的缺点与改进

## 1）K 近邻算法的优缺点

不同类别的样本点，分布在空间的不同区域。K 近邻是基于空间距离较近的样本类别来进行分类，本质上是对于特征空间的划分。

![](img/757205886d8d4aa77603595bc1613333.png)

*   优点：精度高、对异常值不敏感、无数据输入假定。

*   缺点：计算复杂度高、空间复杂度高。

*   适用数据范围：数值型和标称型。

## 2）K 近邻算法的核心要素：距离度量准则

近邻算法能用一种有效的方式隐含的计算决策边界。另外，它也可以显式的计算决策边界，以及有效率的这样做计算，使得计算复杂度是边界复杂度的函数。**K 近邻算法依赖于空间中相近的点做类别判断，判断距离远近的度量标准非常重要**。

距离的度量标准，对很多算法来说都是核心要素（比如无监督学习的 [聚类算法](http://www.showmeai.tech/article-detail/197) 也很大程度依赖距离度量），也对其结果有很大的影响。

![](img/81ddf7013e05353248523f3594bb423e.png)

**Lp 距离**（**又称闵可夫斯基距离，Minkowski Distance**）不是一种距离，而是**一组距离**的定义。

*   参数**p=1**时为**曼哈顿距离**（又称 L1 距离或程式区块距离），表示两个点在标准坐标系上的绝对轴距之和。

*   参数**p=2**时为**欧氏距离**（又称 L2 距离或欧几里得度量），是直线距离常见的两点之间或多点之间的距离表示法。

*   参数**p→∞**时，就是**切比雪夫距离**（各坐标数值差的最大值）。

## 3）K 近邻算法的核心要素：K 的大小

对于 KNN 算法而言，K 的大小取值也至关重要，如果选择较小的 K 值，意味着整体模型变得复杂（模型容易发生过拟合），模型学习的近似误差（approximation error）会减小，但估计误差（estimation error）会增大。

如果选择较大的 K 值，就意味着整体的模型变得简单，减少学习的估计误差，但缺点是学习的近似误差会增大。

> 在实际的应用中，一般采用一个比较小的 K 值。并采用交叉验证的方法，选取一个最优的 K 值。

## 4）K 近邻算法的缺点与改进

### （1）缺点

观察下面的例子，我们看到，对于样本 X，通过 KNN 算法，我们显然可以得到 X 应属于红色类别。但对于样本 Y，KNN 算法判定的结果是 Y 应属于蓝色类别，然而从距离上看 Y 和红色的批次样本点更接近。因此，原始的 KNN 算法只考虑近邻不同类别的样本数量，而忽略掉了距离。

![](img/324ef9b4da0ebddc320c18b14204306a.png)

除了上述缺点，KNN 还存在如下缺点：

*   **样本库容量依赖性较强对 KNN 算法在实际应用中的限制较大**：有不少类别无法提供足够的训练样本，使得 KNN 算法所需要的相对均匀的特征空间条件无法得到满足，使得识别的误差较大。

*   **K 值的确定**：KNN 算法必须指定 K 值，K 值选择不当则分类精度不能保证。

### （2）改进方法

![](img/ed8557557e3d797a7a12c7c6fe83c062.png)

**加快 KNN 算法的分类速度**。

*   浓缩训练样本当训练样本集中样本数量较大时，为了减小计算开销，可以对训练样本集进行编辑处理，即从原始训练样本集中选择最优的参考子集进行 K 近邻寻找，从而减少训练样本的存储量和提高计算效率。

*   加快 K 个最近邻的搜索速度这类方法是通过快速搜索算法，在较短时间内找到待分类样本的 K 个最近邻。

**对训练样本库的维护**。

*   对训练样本库进行维护以满足 KNN 算法的需要，包括对训练样本库中的样本进行添加或删除，采用适当的办法来保证空间的大小，如符合某种条件的样本可以加入数据库中，同时可以对数据库库中已有符合某种条件的样本进行删除。从而保证训练样本库中的样本提供 KNN 算法所需要的相对均匀的特征空间。

# 5.案例介绍

假如一套房子打算出租，但不知道市场价格，可以根据房子的规格（面积、房间数量、厕所数量、容纳人数等），在已有数据集中查找相似（K 近邻）规格的房子价格，看别人的相同或相似户型租了多少钱。

![](img/d376a9d5dc4eb28a7123f351f43287ec.png)

**分类过程**：已知的数据集中，每个已出租住房都有房间数量、厕所数量、容纳人数等字段，并有对应出租价格。将预计出租房子数据与数据集中每条记录比较计算欧式距离，取出距离最小的 5 条记录，将其价格取平均值，可以将其看做预计出租房子的市场平均价格。

注意：

*   最好不要将所有数据全部拿来测试，需要**分出训练集和测试集**,具体划分比例按数据集确定。

*   理想情况下，数据集中每个字段取值范围都相同。但实际上这是几乎不可能的，如果计算时直接用原数数据计算，则会造成较大训练误差。所以需要**对各列数据进行标准化或归一化操作，尽量减少不必要的训练误差**。

*   **数据集中非数值类型的字段需要转换**，替换掉美元$符号和千分位逗号。

## ShowMeAI 相关文章推荐

*   [1.机器学习基础知识](http://www.showmeai.tech/article-detail/185)
*   [2.模型评估方法与准则](http://www.showmeai.tech/article-detail/186)
*   [3.KNN 算法及其应用](http://www.showmeai.tech/article-detail/187)
*   [4.逻辑回归算法详解](http://www.showmeai.tech/article-detail/188)
*   [5.朴素贝叶斯算法详解](http://www.showmeai.tech/article-detail/189)
*   [6.决策树模型详解](http://www.showmeai.tech/article-detail/190)
*   [7.随机森林分类模型详解](http://www.showmeai.tech/article-detail/191)
*   [8.回归树模型详解](http://www.showmeai.tech/article-detail/192)
*   [9.GBDT 模型详解](http://www.showmeai.tech/article-detail/193)
*   [10.XGBoost 模型最全解析](http://www.showmeai.tech/article-detail/194)
*   [11.LightGBM 模型详解](http://www.showmeai.tech/article-detail/195)
*   [12.支持向量机模型详解](http://www.showmeai.tech/article-detail/196)
*   [13.聚类算法详解](http://www.showmeai.tech/article-detail/197)
*   [14.PCA 降维算法详解](http://www.showmeai.tech/article-detail/198)

## ShowMeAI 系列教程推荐

*   [图解 Python 编程：从入门到精通系列教程](http://www.showmeai.tech/tutorials/56)
*   [图解数据分析：从入门到精通系列教程](http://www.showmeai.tech/tutorials/33)
*   [图解 AI 数学基础：从入门到精通系列教程](http://showmeai.tech/tutorials/83)
*   [图解大数据技术：从入门到精通系列教程](http://www.showmeai.tech/tutorials/84)
*   [图解机器学习算法：从入门到精通系列教程](http://www.showmeai.tech/tutorials/34)

![](img/3cc6458c1314658aae68e19504d1a684.png)