# 斯坦福 NLP 课程 | 第 14 讲 - Transformers 自注意力与生成模型

> 原文：[`blog.csdn.net/ShowMeAI/article/details/124667563`](https://blog.csdn.net/ShowMeAI/article/details/124667563)

![](img/aebcb87e9c0384c772533cd04ec6dd1d.png)

*   作者：[韩信子](https://github.com/HanXinzi-AI)@[ShowMeAI](http://www.showmeai.tech/)，路遥@[ShowMeAI](http://www.showmeai.tech/)，奇异果@[ShowMeAI](http://www.showmeai.tech/)
*   [教程地址](http://www.showmeai.tech/tutorials/36)：[`www.showmeai.tech/tutorials/36`](http://www.showmeai.tech/tutorials/36)
*   [本文地址](http://www.showmeai.tech/article-detail/251)：[`www.showmeai.tech/article-detail/251`](http://www.showmeai.tech/article-detail/251)
*   声明：版权所有，转载请联系平台与作者并注明出处
*   收藏[ShowMeAI](http://www.showmeai.tech/)查看更多精彩内容

* * *

![Transformers 自注意力与生成模型](img/473e842b198a2bbd871f3c4b94995aaa.png)

[ShowMeAI](http://www.showmeai.tech/)为**斯坦福 CS224n**《自然语言处理与深度学习(Natural Language Processing with Deep Learning)》课程的全部课件，做了**中文翻译和注释**，并制作成了 GIF 动图！视频和课件等资料的获取方式见**文末**。

* * *

# 引言

![自注意力与生成模型](img/d333dd3078185a8d94aeb3b10e1e5251.png)

## 学习变长数据的表示

![学习变长数据的表示](img/32b04887c57e5dc5d7c951bb293570bc.png)

*   学习变长数据的表示，这是序列学习的基本组件
*   序列学习包括 NMT，text summarization，QA，···

# 1.循环神经网络(RNN)

![循环神经网络(RNN)](img/ac86de52b513167c73474db5f2f83560.png)

*   通常使用 RNN 学习变长的表示
*   RNN 本身适合句子和像素序列
*   LSTMs, GRUs 和其变体在循环模型中占主导地位

![循环神经网络(RNN)](img/6c79485b7eb42a76e840c8a5db42beb9.png)

![循环神经网络(RNN)](img/985b23d030ada47a610d9e7bf4b73a81.png)

*   但是序列计算抑制了并行化
*   没有对长期和短期依赖关系进行显式建模
*   我们想要对层次结构建模
*   RNNs(顺序对齐的状态)看起来很浪费！

# 2.卷积神经网络(CNN)

![卷积神经网络(CNN)](img/279c84b4100eb3894c2ac643be43e9f9.png)

![卷积神经网络(CNN)](img/d7358871991d5b9ac6f0d3c478ec4577.png)

*   并行化(每层)很简单
*   利用局部依赖
*   不同位置的交互距离是线性或是对数的
*   远程依赖需要多层

# 3.Attention 注意力

![Attention 注意力](img/8de79f2dad32c9302745049f40f37a98.png)

*   NMT 中，编码器和解码器之间的 Attention 是至关重要的
*   为什么不把注意力用于表示呢？

## 3.1 自注意力

![自注意力](img/c4570d3341ab8fb66802d80060d9d2c2.png)

*   自注意力机制

# 4.文本生成

![文本生成](img/518ed4bb8125500c6c798d11621d04ff.png)

## 4.1 自注意力

![自注意力](img/53e9863cf08e6995286c04d46bd89027.png)

*   任何两个位置之间的路径长度都是常数级别的
*   门控 / 乘法 的交互
*   可以并行化（每层）
*   可以完全替代序列计算吗？

## 4.2 既有成果

![Previous Work 既有成果](img/79097ab13b774e8646daf3df7a17bdcc.png)

*   Classification & regression with self-attention:
    *   Parikh et al.(2016), Lin et al. (2016)
*   Self-attention with RNNs:
    *   Long et al.(2016), Shao, Gows et al.(2017)
*   Recurrent attention:
    *   Sukhbaatar et al.(2015)

## 4.3 Transformer

![The Transformer](img/eb4dcdc78dd48e3e9d87d46bd0045bd0.png)

*   Transformer 结构

## 4.4 编码器与解码器的自注意力

![编码器与解码器的自注意力](img/32e74ad0732c9d926e4c6716b7e2529c.png)

*   编码器的自注意力
*   解码器的自注意力

## 4.5 Attention is Cheap!

![Attention is Cheap!](img/ae6199a47700e86ee8f314a790cea411.png)

*   由于计算只涉及到两个矩阵乘法，所以是序列长度的平方
*   当维度比长度大得多的时候，非常有效

## 4.6 注意力：加权平均

![注意力：加权平均](img/3f3e87f1f938a6c6cef5dac0b3d81331.png)

## 4.7 自注意力

![自注意力](img/2c221b3f37bbbcb189e2e1d5b79bc066.png)

*   上例中，我们想要知道谁对谁做了什么，通过卷积中的多个卷积核的不同的线性操作，我们可以分别获取到 who, did what, to whom 的信息。

*   但是对于 Attention 而言，如果只有一个 Attention layer，那么对于一句话里的每个词都是同样的线性变换，不能够做到在不同的位置提取不同的信息
*   {>>这就是多头注意力的来源，灵感来源于 CNN 中的多个卷积核的设计<<}

*   Who，Did What，To Whom，分别拥有注意力头

*   将注意力层视为特征探测器
*   可以并行完成
*   为了效率，减少注意力头的维度，并行操作这些注意力层，弥补了计算差距

## 4.8 卷积和多头注意力

![卷积和多头注意力](img/47179f3728d9c7f6e85456606116aa26.png)

*   Different linear transformations by relative position.
*   Parallel attention layers with different linear transformations on input and output.

# 5.Results

![Results](img/ed4153ca09db4b3f92f9b7a9d8b0ef3b.png)

## 5.1 机器翻译: WMT-2014 BLEU

![Machine Translation : WMT-2014 BLEU](img/ab7bcd2d61a8fb46c9fd6eebc59d750d.png)

*   但我们并不一定比 LSTM 取得了更好的表示，只是我们更适合 SGD，可以更好的训练
*   我们可以对任意两个词之间构建连接

# 6.框架

![框架](img/eab91dafba7066c207ae334acf8baa8f.png)

## 6.1 残差连接的必要性

![残差连接的必要性](img/e1c3ed6744144b78793aa7a6c6f4530e.png)

![残差连接的必要性](img/84c09c48268b01efce8f69aea498008d.png)

*   残差连接结构
*   位置信息最初添加在了模型的输入处，通过残差连接将位置信息传递到每一层，可以不需要再每一层都添加位置信息

## 6.2 训练细节

![训练细节](img/2e55301e348e91f5ee7a67e36c47da9d.png)

*   ADAM 优化器，同时使用了学习率预热 (warmup + exponential decay)
*   每一层在添加残差之前都会使用 dropout
*   Layer-norm/层归一化
*   有些实验中使用了 Attention dropout
*   Checkpoint-averaging 检查点平均处理
*   Label smoothing 标签平滑
*   Auto-regressive decoding with beam search and length biasing 使用集束搜索和 length biasing 的自回归解码
*   ……

## 6.3 What Matters?

![What Matters?](img/e374fa04a62aaf35b71e40d8333c7ece.png)

## 6.4 Generating Wikipedia by Summarizing Long Sequences

![Generating Wikipedia by Summarizing Long Sequences](img/6882e0884a70929e0a0e6486cfc6cda2.png)

# 7.自相似度，图片与音乐生成

![自相似度，图片与音乐生成](img/4dec75d9df63ae305d564d1697415ea4.png)

## 7.1 自相似度，图片与音乐生成

![自相似度，图片与音乐生成](img/c97e69ea8ad08ef5bd95204864f2fde5.png)

## 7.2 基于概率分布的图像生成

![基于概率分布的图像生成](img/4ab19de2b84b0f7d141dd1736f15a521.png)

*   模拟像素的联合分布
*   把它变成一个序列建模问题
*   分配概率允许度量泛化

![基于概率分布的图像生成](img/1dbdd640258e448c69acd891373350db.png)

*   RNNs 和 CNNs 是最先进的(PixelRNN, PixelCNN)
*   incorporating gating CNNs 现在在效果上与 RNNs 相近
*   由于并行化，CNN 要快得多

![ 基于概率分布的图像生成](img/7b44af06a67bc98904c5144a509b7806.png)

*   图像的长期依赖关系很重要(例如对称性)
*   可能随着图像大小的增加而变得越来越重要
*   使用 CNNs 建模长期依赖关系需要两者之一
    *   多层可能使训练更加困难
    *   大卷积核参数/计算成本相应变大

## 7.3 自相似性的研究

![自相似性的研究](img/94bf59a0feb7cfd4535e822983452fd2.png)

*   自相似性的研究案例

## 7.4 非局部均值

![Non-local Means](img/28304607c2f19b095f207d13c6dff92a.png)

*   A Non-local Algorithm for Image Denoising (Buades, Coll, and Morel. CVPR 2005)
*   Non-local Neural Networks (Wang et al., 2018)

## 7.5 既有工作

![既有工作](img/0a20594c2d5a4e2f1d822a5e812e62e6.png)

*   Self-attention:
    *   Parikh et al. (2016), Lin et al. (2016), Vaswani et al. (2017)
*   Autoregressive Image Generation:
    *   A Oord et al. (2016), Salimans et al. (2017)

## 7.6 自注意力

![自注意力](img/d5f3fd468d4eb7e956472d64b373806f.png)

## 7.7 图像 Transformer

![图像 Transformer](img/9836b814d7f56cefa0c080f5bb989329.png)

![图像 Transformer](img/bc95ae5457622cf1ad2b90cbeb05c5c7.png)

## 7.8 Attention is Cheap if length<<dim!

![Attention is Cheap if length<<dim!](img/53b2641e9013aa7ed627b590355c3353.png)

## 7.9 Combining Locality with Self-Attention

![Combining Locality with Self-Attention](img/bd14573dabbbf6b9f6d0f8254d572968.png)

*   将注意力窗口限制为局部范围
*   由于空间局部性，这在图像中是很好的假设

## 7.10 局部 1 维和 2 维注意力

![局部 1 维和 2 维注意力](img/41d6cf299ad55cc77e28d56f42f3d984.png)

## 7.11 图像 Transformer 层

![图像 Transformer 层](img/f4c4219f5a9946b2b10d993d8f8f289a.png)

## 7.12 Task

![Task](img/a3fc81acb9c855b87888c25b689bd222.png)

## 7.13 Results

![Results](img/4a473ad107e0cc8a0c3be9841521b850.png)

*   lmage Transformer
*   Parmar , Vaswani",Uszkoreit, Kaiser, Shazeer,Ku, and Tran.ICML 2018

## 7.14 无约束图像生成

![无约束图像生成](img/636b9c9e3d434bf668d2a1be4248c81e.png)

## 7.15 Cifar10 样本

![Cifar10 样本](img/f2287a67c1990e47e7964b66ebcc8ebe.png)

## 7.16 CelebA 超分辨率重建

![CelebA 超分辨率重建](img/b9e6877d168ea61eef9c04277714a940.png)

## 7.17 条件图片生成

![条件图片生成](img/faf12a342b31be7ebfb8502ddb97884c.png)

# 8.相对自注意力音乐生成

![相对自注意力音乐生成](img/c3d3cfbe07224e9c502cb92e7e82b745.png)

## 8.1 音乐和语言的原始表征

![音乐和语言的原始表征](img/30487f62be5aad93021d2cbf9fa8f806.png)

## 8.2 音乐语言模型

![音乐语言模型](img/539e034f9e36f939894f56c1515e0cf5.png)

*   传统的 RNN 模型需要将长序列嵌入到固定长度的向量中

## 8.3 Continuations to given initial motif

![Continuations to given initial motif](img/6e85f4a6dc5a0247dd4ed4d50a748d86.png)

## 8.4 音乐自相似度

![Self-Similarity in Music](img/261f5d25b3abc7a416d0f05408e923ac.png)

*   给定一段音乐并生成后续音乐
*   不能直接去重复过去的片段
*   难以处理长距离

## 8.5 注意力：加权平均

![Attention : a weighted average](img/de6f415ba9e9ac2307173cbff9d5ae56.png)

*   移动的固定过滤器捕获相对距离
*   Music Transformer 使用平移不变性来携带超过其训练长度的关系信息，进行传递
*   Different linear transformations by relative position.

## 8.6 近观相对注意力

![近观相对注意力](img/667d7f101848497f8eac485e2a3b23b4.png)

*   相对注意力
*   位置之间的相关性
*   但是音乐中的序列长度通常非常长

## 8.7 机器翻译

![机器翻译](img/35e6ce39d3712f9cefd0db58d38f0034.png)

## 8.8 既有成果

![既有成果](img/9d213047bc44b925a8d193f4ee2860e8.png)

## 8.9 Our formulation

![Our formulation](img/39927369db79014d41d65f19e78956d5.png)

*   将相对距离转化为绝对距离

## 8.10 Goal of skewing procedure

![Goal of skewing procedure](img/efa5594dd28b559065ac5f708d98212a.png)

## 8.11 Skewing to reduce relative memoryfrom O(L2D) to O(LD)

![Skewing to reduce relative memoryfrom O(L2D) to O(LD)](img/8f93d191f4cd735a9d59958853a773ad.png)

## 8.12 AJazz sample from Music Transformer

![AJazz sample from Music Transformer](img/52dceb1fbf4345a188bc5dbf15a45386.png)

## 8.13 Convolutions and Translational Equivariance

![Convolutions and Translational Equivariance](img/5c20fd93546ab33e4baa2c64edd3f6a2.png)

## 8.14 Relative Attention And Graphs

![Relative Attention And Graphs](img/ecd670767dec99e0c5c57f32d5ce779a.png)

## 8.15 Message Passing Neural Networks

![Message Passing Neural Networks](img/d9f5adb8002bc64678154304b1e416c6.png)

## 8.16 多塔结构

![多塔结构](img/80e349f9f0f7b31bf230393c92f24a94.png)

## 8.17 图工具库

![图工具库](img/86ccaf2601c5f1c2e97a21e7a6075a09.png)

## 8.18 自注意力

![ 自注意力](img/2b06ec537a630822601ba054ace58e7f.png)

*   任意两个位置之间的路径长度是常数级的
*   没有边界的内存
*   易于并行化
*   对自相似性进行建模
*   相对注意力提供了表达时间、equivariance，可以自然延伸至图表

## 8.19 热门研究领域

![热门研究领域](img/6aff142cf02f362067d179a7ae3d08d5.png)

*   Non autoregressive transformer (Gu and Bradbury et al., 2018)
*   Deterministic Non-Autoregressive Neural Sequence Modeling by lterative Refinement(Lee,Manismov, and Cho,2018)
*   Fast Decoding in Sequence Models Using Discrete Latent Variables (ICML 2018)Kaiser, Roy, Vaswani, Pamar, Bengio, Uszkoreit, Shazeer
*   Towards a Better Understanding of Vector Quantized AutoencodersRoy,Vaswani, Parmar,Neelakantan, 2018
*   Blockwise Parallel Decoding For Deep Autogressive Models (NeurlPS 2019)Stern, Shazeer,Uszkoreit,

# 9.迁移学习

![迁移学习](img/41329328fb8f251f5470fe95a0d35146.png)

![迁移学习](img/a8719eb8eda8c1a433a777c664bcd502.png)

# 10.优化&大模型

![优化&大模型](img/25697b9c33eadc86d5c9a9c816c63ba5.png)

![优化&大模型](img/fe806610bb539f83bb9e4541d11da6fa.png)

*   Adafactor: Adaptive Learning Rates with Sublinear Memory Cost(ICML 2018).Shazeer,Stern.
*   Memory-Efficient Adaptive Optimization for Large-Scale Learning (2019).Anil,Gupta, Koren, Singer.
*   Mesh-TensorFlow: Deep Learning for Supercomputers (NeurlPS 2019).
*   Shazeer, Cheng,Parmar,Tran, Vaswani, Koanantakool,Hawkins,Lee,Hong,Young, Sepassi, Hechtman) Code (5 billion parameters)

# 11.自注意力其他研究与应用

![自注意力其他研究与应用](img/307fa51dd1d02c48f91299105d516d93.png)

![自注意力其他研究与应用](img/3dcddfd050afa7a5805cc8b36886279e.png)

*   Generating Wikipedia by Summarizing Long sequences.(ICLR 2018). Liu,Saleh,Pot, Goodrich, Sepassi, Shazeer, Kaiser.
*   Universal Transformers (ICLR 2019). Deghiani*, Gouws*,Vinyals, Uszkoreit,Kaiser.
*   Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context(2019). Dai, Yang,Yang,Carbonell,Le, Salakhutdinov.
*   A Time-Restricted Self-Attention Layer for ASR (ICASSP 2018).Povey,Hadian,Gharemani,Li, Khudanpur.
*   Character-Level Language Modeling with Deeper Self-Attention (2018).Roufou*, Choe*, Guo*, Constant* , Jones*

# 12.未来的工作研究方向

![未来的工作研究方向](img/0d7f966858d72a9c5346d3006dbc219c.png)

![未来的工作研究方向](img/8a2256a58ec6d4468db60903fa589bd0.png)

*   Self-supervision and classification for images and video
*   Understanding Transfer

![未来](img/c3b85492c3a6b607626fb3a245997de2.png)

# 13.视频教程

可以点击 [**B 站**](https://www.bilibili.com/video/BV1Yo4y1D7FW?p=14) 查看视频的【双语字幕】版本

[`player.bilibili.com/player.html?aid=376755412&page=14`](https://player.bilibili.com/player.html?aid=376755412&page=14)

【双语字幕+资料下载】斯坦福 CS224n | 深度学习与自然语言处理(2019·全 20 讲)

# 14.参考资料

*   [本讲带学的**在线阅翻页本**](https://blog.showmeai.tech/cs224n/lecture14-Transformers-and-Self-Attention-For-Generative-Models#/)
*   [《斯坦福 CS224n 深度学习与自然语言处理》**课程学习指南**](https://blog.showmeai.tech/cs224n/)
*   [《斯坦福 CS224n 深度学习与自然语言处理》**课程大作业解析**](https://github.com/ShowMeAI-Hub/awesome-AI-courses-notes-cheatsheets/tree/main/CS224n-Natural-Language-Processing-with-Deep-Learning/assignment-solutions)
*   [【**双语字幕视频**】斯坦福 CS224n | 深度学习与自然语言处理(2019·全 20 讲)](https://www.bilibili.com/video/BV1Yo4y1D7FW)
*   [**Stanford 官网** | CS224n: Natural Language Processing with Deep Learning](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/)

# [**ShowMeAI**](http://www.showmeai.tech)系列教程推荐

*   [大厂技术实现 | 推荐与广告计算解决方案](http://www.showmeai.tech/tutorials/50)
*   [大厂技术实现 | 计算机视觉解决方案](http://www.showmeai.tech/tutorials/51)
*   [大厂技术实现 | 自然语言处理行业解决方案](http://www.showmeai.tech/tutorials/52)
*   [图解 Python 编程：从入门到精通系列教程](http://www.showmeai.tech/tutorials/56)
*   [图解数据分析：从入门到精通系列教程](http://www.showmeai.tech/tutorials/33)
*   [图解 AI 数学基础：从入门到精通系列教程](http://www.showmeai.tech/tutorials/83)
*   [图解大数据技术：从入门到精通系列教程](http://www.showmeai.tech/tutorials/84)
*   [图解机器学习算法：从入门到精通系列教程](http://www.showmeai.tech/tutorials/34)
*   [机器学习实战：手把手教你玩转机器学习系列](http://www.showmeai.tech/tutorials/41)
*   [深度学习教程 | 吴恩达专项课程 · 全套笔记解读](http://www.showmeai.tech/tutorials/35)
*   [自然语言处理教程 | 斯坦福 CS224n 课程 · 课程带学与全套笔记解读](http://www.showmeai.tech/tutorials/36)

# NLP 系列教程文章

*   [NLP 教程(1)- 词向量、SVD 分解与 Word2vec](http://showmeai.tech/article-detail/230)
*   [NLP 教程(2)- GloVe 及词向量的训练与评估](http://showmeai.tech/article-detail/232)
*   [NLP 教程(3)- 神经网络与反向传播](http://showmeai.tech/article-detail/234)
*   [NLP 教程(4)- 句法分析与依存解析](http://www.showmeai.tech/article-detail/237)
*   [NLP 教程(5)- 语言模型、RNN、GRU 与 LSTM](http://www.showmeai.tech/article-detail/239)
*   [NLP 教程(6)- 神经机器翻译、seq2seq 与注意力机制](http://www.showmeai.tech/article-detail/242)
*   [NLP 教程(7)- 问答系统](http://www.showmeai.tech/article-detail/245)
*   [NLP 教程(8)- NLP 中的卷积神经网络](http://www.showmeai.tech/article-detail/247)
*   [NLP 教程(9)- 句法分析与树形递归神经网络](http://www.showmeai.tech/article-detail/255)

# 斯坦福 CS224n 课程带学详解

*   [斯坦福 NLP 课程 | 第 1 讲 - NLP 介绍与词向量初步](http://showmeai.tech/article-detail/231)
*   [斯坦福 NLP 课程 | 第 2 讲 - 词向量进阶](http://showmeai.tech/article-detail/233)
*   [斯坦福 NLP 课程 | 第 3 讲 - 神经网络知识回顾](http://showmeai.tech/article-detail/235)
*   [斯坦福 NLP 课程 | 第 4 讲 - 神经网络反向传播与计算图](http://showmeai.tech/article-detail/236)
*   [斯坦福 NLP 课程 | 第 5 讲 - 句法分析与依存解析](http://www.showmeai.tech/article-detail/238)
*   [斯坦福 NLP 课程 | 第 6 讲 - 循环神经网络与语言模型](http://www.showmeai.tech/article-detail/240)
*   [斯坦福 NLP 课程 | 第 7 讲 - 梯度消失问题与 RNN 变种](http://www.showmeai.tech/article-detail/241)
*   [斯坦福 NLP 课程 | 第 8 讲 - 机器翻译、seq2seq 与注意力机制](http://www.showmeai.tech/article-detail/243)
*   [斯坦福 NLP 课程 | 第 9 讲 - cs224n 课程大项目实用技巧与经验](http://www.showmeai.tech/article-detail/244)
*   [斯坦福 NLP 课程 | 第 10 讲 - NLP 中的问答系统](http://www.showmeai.tech/article-detail/246)
*   [斯坦福 NLP 课程 | 第 11 讲 - NLP 中的卷积神经网络](http://www.showmeai.tech/article-detail/248)
*   [斯坦福 NLP 课程 | 第 12 讲 - 子词模型](http://www.showmeai.tech/article-detail/249)
*   [斯坦福 NLP 课程 | 第 13 讲 - 基于上下文的表征与 NLP 预训练模型](http://www.showmeai.tech/article-detail/250)
*   [斯坦福 NLP 课程 | 第 14 讲 - Transformers 自注意力与生成模型](http://www.showmeai.tech/article-detail/251)
*   [斯坦福 NLP 课程 | 第 15 讲 - NLP 文本生成任务](http://www.showmeai.tech/article-detail/252)
*   [斯坦福 NLP 课程 | 第 16 讲 - 指代消解问题与神经网络方法](http://www.showmeai.tech/article-detail/253)
*   [斯坦福 NLP 课程 | 第 17 讲 - 多任务学习(以问答系统为例)](http://www.showmeai.tech/article-detail/254)
*   [斯坦福 NLP 课程 | 第 18 讲 - 句法分析与树形递归神经网络](http://www.showmeai.tech/article-detail/256)
*   [斯坦福 NLP 课程 | 第 19 讲 - AI 安全偏见与公平](http://www.showmeai.tech/article-detail/257)
*   [斯坦福 NLP 课程 | 第 20 讲 - NLP 与深度学习的未来](http://www.showmeai.tech/article-detail/258)

![](img/d762026cbf142061ada32b69ff2c765e.png)