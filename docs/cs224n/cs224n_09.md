# 斯坦福 NLP 课程 | 第 9 讲 - cs224n 课程大项目实用技巧与经验

> 原文：[`blog.csdn.net/ShowMeAI/article/details/124585492`](https://blog.csdn.net/ShowMeAI/article/details/124585492)

![](img/aebcb87e9c0384c772533cd04ec6dd1d.png)

作者：[韩信子](https://github.com/HanXinzi-AI)@[ShowMeAI](http://www.showmeai.tech/)，路遥@[ShowMeAI](http://www.showmeai.tech/)，奇异果@[ShowMeAI](http://www.showmeai.tech/)
[教程地址](http://www.showmeai.tech/tutorials/36)：[`www.showmeai.tech/tutorials/36`](http://www.showmeai.tech/tutorials/36)
[本文地址](http://www.showmeai.tech/article-detail/244)：[`www.showmeai.tech/article-detail/244`](http://www.showmeai.tech/article-detail/244)
声明：版权所有，转载请联系平台与作者并注明出处

收藏[ShowMeAI](http://www.showmeai.tech/)查看更多精彩内容

* * *

![cs224n 课程大项目实用技巧与经验](img/92ec103753fd2bb138d31551c9ec0642.png)
[ShowMeAI](http://www.showmeai.tech/)为**斯坦福 CS224n**《自然语言处理与深度学习(Natural Language Processing with Deep Learning)》课程的全部课件，做了**中文翻译和注释**，并制作成了 GIF 动图！视频和课件等资料的获取方式见**文末**。

* * *

# 引言

![课程大项目实用技巧与经验](img/e8eb518189bfc71ffe920225e62d529a.png)

## 授课计划

![授课计划](img/279e2d7073d4f80eb57bf911aee67b24.png)

*   Final project types and details; assessment revisited / 大项目细节
*   Finding research topics; a couple of examples / 寻找研究主题
*   Finding data / 项目数据
*   Review of gated neural sequence models / 门控神经序列模型回顾
*   A couple of MT topics / 机器翻译主题
*   Doing your research / 研究方式
*   Presenting your results and evaluation / 结果呈现和评估

# 1.大项目细节

## 1.1 Course work and grading policy

![Course work and grading policy](img/7577e129334076a428728d1b5fba70be.png)

## 1.2 Mid-quarter feedback survey

![Mid-quarter feedback survey](img/10640a818ab8848f2a61a28bffb03049.png)

## 1.3 最终项目

![The Final Project](img/61221c14fa3b7c26c3bee94cf944db94.png)

## 1.4 默认最终项目：基于 SQuAD 的问答系统

![The Default Final Project](img/b0808ec181cba09ab9cdf6396d1d359f.png)

## 1.5 什么情况下选择默认项目

![Why Choose The Default Final Project?](img/83a7e6c0fd839b6b8d86d504df2d486b.png)

## 1.6 本节内容是相关的

![This lecture is still relevant ... Even if doing DFP](img/0d3da10180bb4f4ea70c5d614c18439f.png)

*   At a lofty level
    *   了解一些关于做研究的知识是有好处的
*   At a prosaic level
    *   我们将接触到:
        *   基线
        *   基准
        *   评估
        *   错误分析
        *   论文写作
    *   这也是默认最终项目的一大特点

## 1.7 什么情况下选择 Custom Final Project

![Why Choose The Custom Final Project?](img/eef149d9c339395eeebb266af7eeda4b.png)

## 1.8 项目提议与候选

![Project Proposal – from everyone 5%](img/ea72c5603a0bb968d893df8fd9673250.png)

## 1.9 项目进度

![Project Milestone – from everyone 5%](img/86b643e0f8795e5bb177fca9c1dc2174.png)

# 2.寻找研究主题

## 2.1 寻找研究主题

![Finding Research Topics](img/1b5c478760a2947286eebb944e68948a.png)

所有科学的两个基本出发点

*   [**钉子**]从一个(领域)感兴趣的问题开始，并试图找到比目前已知的/使用的更好的方法来解决它。
*   [**锤子**]从一个感兴趣的技术方法开始，找出扩展或改进它或应用它的好方法

## 2.2 项目类型

![Project types](img/894a8fd4b9578514b80ed10c12c41999.png)

这不是一个详尽的列表，但大多数项目都是其中之一

*   1.找到感兴趣的应用程序/任务，探索如何有效地接近/解决它，通常应用现有的神经网络模型

*   2.实现了一个复杂的神经结构，并在一些数据上展示了它的性能

*   3.提出一种新的或变异的神经网络模型，并探讨其经验上的成功

*   4.分析项目。分析一个模型的行为：它如何表示语言知识，或者它能处理什么样的现象，或者它犯了什么样的错误

*   5.稀有的理论项目：显示模型类型、数据或数据表示的一些有趣的、重要的属性

## 2.3 示例项目：Deep Poetry-诗歌生成

![Deep Poetry : Word-Level and Character-level Language Models for Shakespearean Sonnet Generation](img/388bb6951f25578e4647b0b9fa4bbb5e.png)

## 2.4 项目示例：Implementation and Optimization of Differentiable Neural Computers

![Implementation and Optimization of Differentiable Neural Computers](img/a7d6037be3b99001b1ed53e2d536e00f.png)

## 2.5 项目示例：Improved Learning through Augmenting the Loss

![Improved Learning through Augmenting the Loss](img/b2fc896f71343fe1c7d1b8490f53bbad.png)

## 2.6 项目示例：Word2bits - Quantized Word Vectors

![Word2bits - Quantized Word Vectors](img/39dff3e6b6bf939e5184f141944f6321.png)

## 2.7 如何寻找有趣的项目应用点

![How to find an interesting place to start?](img/40b1999922b8f39a8a123bea736b4602.png)

![How to find an interesting place to start?](img/74e3f7283da2029734fe4b241cdf653b.png)

## 2.8 如何能够对算法优化突破

![Want to beat the state of the art on something?](img/19c4396292668ee5b772b25350e2cb6c.png)

## 2.9 寻找一个主题

![Finding a topic](img/8403b3a10ae34bccd5de98be89fa2ffc.png)

## 2.10 项目注意点

![Must-haves (for most* custom final projects)](img/4efeab6a47a1a84da4758f8f25fef99b.png)

*   合适的数据
    *   通常目标:10000 +标记的例子里程碑
*   可行的任务
*   自动评估指标
*   NLP 是项目的核心

# 3.项目数据

## 3.1 寻找合适的数据

![Finding data](img/6193ebcc92d9cea23eac1d06fedc5e65.png)

*   有些人会为一个项目收集他们自己的数据
    *   你可能有一个使用“无监督”数据的项目
    *   你可以注释少量的数据
    *   你可以找到一个网站，有效地提供注释，如喜欢，明星，评级等

*   有些人使用现有的研究项目或公司的数据
    *   如果你可以提供提交、报告等数据样本

*   大多数人使用现有的，由以前的研究人员建立的数据集
    *   你有一个快速的开始，有明显的前期工作和基线

## 3.2 数据：Linguistic Data Consortium

![Linguistic Data Consortium](img/ceca55a5a785bf4d4fe10488ea42db21.png)

*   https://catalog.ldc.upenn.edu/
*   https://linguistics.stanford.edu/resources/resources-corpora

## 3.3 机器翻译

![Machine translation](img/f6f56a7d7611f37a768fc4b03c22ecf4.png)

*   http://statmt.org
*   特别要注意各种 WMT 共享任务

## 3.4 依存解析

![Dependency parsing: Universal Dependencies](img/548a020bd6f92d0fcb7e835d62e5528c.png)

*   https://universaldependencies.org

## 3.5 其他

![Many, many more](img/4ac586d921d2b2146402e8496c9b45f7.png)

现在网上有很多其他的数据集可以用于各种各样的目的

*   看 Kaggle
*   看研究论文
*   看数据集列表
    *   https://machinelearningmastery.com/datasets-natural-languageprocessing/
    *   https://github.com/niderhoff/nlp-datasets

# 4.门控神经序列模型回顾

## 4.1 再回顾一下 GRU 和机器翻译

![One more look at gated recurrent units and MT](img/fd03ccae9303e7cb113d314b46f81df7.png)

## 4.2 BPTT 反向传播

![Backpropagation through Time](img/f300ccb42d773c12d24c1f2ca619dbbd.png)

*   梯度消失问题十分严重
*   当梯度趋近于 0 0 0 时，我们无法判断

    *   数据中 t t t 和 t + n t+n t+n 之间不再存在依赖关系
    *   参数设置错误（梯度消失条件） 

*   这是原始转换函数的问题吗？

f ( h t − 1 , x t ) = tanh ⁡ ( W [ x t ] + U h t − 1 + b ) f(h_{t-1}, x_t) = \tanh(W[x_t] + U h_{t-1} + b) f(ht−1​,xt​)=tanh(W[xt​]+Uht−1​+b)

*   有了它，时间导数就会消失

∂ h t + 1 ∂ h t = U ⊤ ∂ tanh ⁡ ( a ) ∂ a \frac{\partial h_{t+1}}{\partial h_{t}}=U^{\top} \frac{\partial \tanh (a)}{\partial a} ∂ht​∂ht+1​​=U⊤∂a∂tanh(a)​

## 4.3 GRU 模型

![Gated Recurrent Unit](img/f7729460532cb18cb09c9af114ed3b6c.png)

*   这意味着错误必须通过所有中间节点反向传播
*   或许我们可以创建快捷连接

![Gated Recurrent Unit](img/d6701ec3d2ac9b2517203e11734bce66.png)

我们可以创建自适应的快捷连接

f ( h t − 1 , x t ) = u t ⊙ h ~ t + ( 1 − u t ) ⊙ h t − 1 f\left(h_{t-1}, x_{t}\right)=u_{t} \odot \tilde{h}_{t}+\left(1-u_{t}\right) \odot h_{t-1} f(ht−1​,xt​)=ut​⊙h~t​+(1−ut​)⊙ht−1​

*   **候选更新** h ~ t = tanh ⁡ ( W [ x t ] + U h t − 1 + b ) \tilde{h}_{t}=\tanh \left(W\left[x_{t}\right]+U h_{t-1}+b\right) h~t​=tanh(W[xt​]+Uht−1​+b)
*   **更新门** u t = σ ( W u [ x t ] + U u h t − 1 + b u ) u_{t}=\sigma\left(W_{u}\left[x_{t}\right]+U_{u} h_{t-1}+b_{u}\right) ut​=σ(Wu​[xt​]+Uu​ht−1​+bu​)
*   ⊙ \odot ⊙ 表示逐元素的乘法

![Gated Recurrent Unit](img/94c449b13000047f4dbf66bfad281cf2.png)

让网络自适应地修剪不必要的连接

f ( h t − 1 , x t ) = u t ⊙ h ~ t + ( 1 − u t ) ⊙ h t − 1 f\left(h_{t-1}, x_{t}\right)=u_{t} \odot \tilde{h}_{t}+\left(1-u_{t}\right) \odot h_{t-1} f(ht−1​,xt​)=ut​⊙h~t​+(1−ut​)⊙ht−1​

*   **候选更新** h ~ t = tanh ⁡ ( W [ x t ] + U ( r t ⊙ h t − 1 ) + b ) \tilde{h}_{t}=\tanh \left(W\left[x_{t}\right]+U\left(r_{t} \odot h_{t-1}\right)+b\right) h~t​=tanh(W[xt​]+U(rt​⊙ht−1​)+b)
*   **重置门** r t = σ ( W r [ x t ] + U r h t − 1 + b r ) r_{t}=\sigma\left(W_{r}\left[x_{t}\right]+U_{r} h_{t-1}+b_{r}\right) rt​=σ(Wr​[xt​]+Ur​ht−1​+br​)
*   **更新门** u t = σ ( W u [ x t ] + U u h t − 1 + b u ) u_{t}=\sigma\left(W_{u}\left[x_{t}\right]+U_{u} h_{t-1}+b_{u}\right) ut​=σ(Wu​[xt​]+Uu​ht−1​+bu​)

![Gated Recurrent Unit](img/dc88fa76754a017757dc2835c8f664aa.png)

![Gated Recurrent Unit](img/57a1f0c31d141808b7eb0cd207708a6e.png)

*   门控循环单位更现实
*   注意，在思想和注意力上有一些重叠

![Gated Recurrent Unit](img/b82a84668fd8b42037a00ae637ee7325.png)

*   两个最广泛使用的门控循环单位：GRU 和 LSTM

4.4 LSTM 模型

![The LSTM](img/e0e0d107927dacb88cb685b3bdbae86f.png)

*   (绿色)LSTM 门的所有操作都可以被遗忘/忽略，而不是把所有的东西都塞到其他所有东西上面
*   (橙色)下一步的非线性更新就像一个 RNN
*   (紫色)这部分是核心（ResNets 也是如此）不是乘，而是将非线性的东西和 c t − 1 c_{t−1} ct−1​ 相加得到 c t c_t ct​。 c t c_t ct​， c t − 1 c_{t−1} ct−1​之间存在线性联络

# 5.机器翻译主题

## 5.1 神经翻译系统的词汇问题

![The large output vocabulary problem in NMT (or all NLG)](img/8f639326ba11d8d128c63f743d4483fe.png)

*   Softmax 计算代价昂贵

## 5.2 文本生成问题

![The word generation problem](img/e6fe3afb00b93231771c322d551ba4fd.png)

*   词汇生成问题
    *   词汇量通常适中:50K

## 5.3 解决方法

![Possible approaches for output](img/771701fbccfde8fee585b3ec422d5c13.png)

*   Hierarchical softmax : tree-structured vocabulary
*   Noise-contrastive estimation : binary classification

*   Train on a subset of the vocabulary at a time; test on a smart on the set of possible translations
    *   每次在词汇表的子集上进行训练，测试时自适应的选择词汇表的子集
    *   Jean, Cho, Memisevic, Bengio. ACL2015

*   Use attention to work out what you are translating
    *   You can do something simple like dictionary lookup
    *   直接复制原句中的生词： “复制”模型

*   More ideas we will get to : Word pieces; char. Models

## 5.4 机器翻译评估

![MT Evaluation – an example of eval](img/92262d0b570c058c60a3d508ba78a7a8.png)

*   人工(最好的!?)
    *   Adequacy and Fluency 充分性和流畅性(5 或 7 尺度)
    *   错误分类
    *   翻译排名比较（例如人工判断两个翻译哪一个更好）

*   在使用 MT 作为子组件的应用程序中进行测试
    *   如问答从外语文件
    *   无法测试翻译的很多方面(例如,跨语言 IR)

*   自动度量
    *   BLEU (双语评价替手)
    *   Others like TER, METEOR, ……

## 5.5 BLEU 评估标准

![BLEU Evaluation Metric](img/08e3840553dfbf9e203f0226d7178076.png)

*   N-gram 精度(得分在 0 和 1 之间)
    *   参考译文中机器译文的 N-gram 的百分比是多少?
    *   一个 n-gram 是由 n 个单词组成的序列
    *   在一定的 n-gram 水平上不允许两次匹配相同的参考译文部分(两个 MT 单词 airport 只有在两个参考单词 airport 时才正确；不能通过输入“the the the the the”来作弊)
    *   也要用 unigrams 来计算单位的精度，等等

*   简洁惩罚 BP
    *   不能只输入一个单词“the”(精确度 1.0!)

*   人们认为要“玩弄”这个系统是相当困难的。例如找到一种方法来改变机器的输出，使 BLEU 上升，但质量不会下降。

*   BLEU 是一个加权的几何平均值，加上一个简洁的惩罚因子
    *   注意：只在语料库级起作用(0 会杀死它)；句子级有一个平滑的变体
*   下图是 n-grams 1-4 的 BLEU 计算公式

## 5.6 BLEU 实战

![BLEU in Action](img/0db11f47e71f28484490ca72d5306e67.png)

## 5.7 多参考翻译-Multiple Reference Translations

![Multiple Reference Translations](img/832ab04aa46f9a64dbf3693b83e12f12.png)

## 5.8 BLEU 预估还不错

![Initial results showed that BLEU predicts human judgments well](img/2ba067d33ac7c5edc405ffdb57337e82.png)

## 5.9 机器翻译自动评估

![Automatic evaluation of MT](img/4c61c7636a9655ee1078453d2b4b4065.png)

*   人们开始优化系统最大化 BLEU 分数
    *   BLEU 分数迅速提高
    *   BLEU 和人类判断质量之间的关系一直下降
    *   MT BLEU 分数接近人类翻译但是他们的真实质量仍然远低于人类翻译

*   想出自动 MT 评估已经成为自己的研究领域
    *   有许多建议:TER, METEOR, MaxSim, SEPIA，我们自己的 RTE-MT
    *   TERpA 是一个具有代表性的，好处理一些词的选择变化的度量

*   MT 研究需要一些自动的度量，以允许快速的开发和评估

# 6.研究方式

## 6.1 项目研究示例

![Doing your research example: Straightforward Class Project: Apply NNets to Task](img/539179d6bd667ed79fd2663b1f12a5a4.png)

*   1.**定义任务**
    *   示例：总结

*   2.**定义数据集**

    *   a) 搜索学术数据集
        *   他们已经有基线
        *   例如 Newsroom Summarization Dataset https://summari.es

*   b) 定义你自己的数据(更难，需要新的基线)
    *   允许连接到你的研究
    *   新问题提供了新的机会
    *   有创意:Twitter、博客、新闻等等。有许多整洁的网站为新任务提供了创造性的机会

![Straightforward Class Project: Apply NNets to Task](img/5874b60ac7afcb9a9c32ad624f1fb777.png)

*   3.**数据集卫生**
    *   开始的时候，分离 devtest and test
    *   接下来讨论更多

*   4.  **定义你的度量**(s)
    *   在线搜索此任务的已建立的度量
    *   摘要: Rouge (Recall-Oriented Understudy for GistingEvaluation) ，它定义了人工摘要的 n-gram 重叠
    *   人工评价仍然更适合于摘要；你可以做一个小规模的人类计算

![Straightforward Class Project: Apply NNets to Task](img/17cd72da1edaa39a07c69b88578fcdf9.png)

*   5.**建立基线**

    *   首先实现最简单的模型(通常对 unigrams、bigrams 或平均字向量进行逻辑回归)
    *   在训练和开发中计算指标
    *   如果度量令人惊讶且没有错误，那么
        *   完成!问题太简单了。需要重启

*   6.**实现现有的神经网络模型**

    *   在训练和开发中计算指标
    *   分析输出和错误
    *   这门课的最低标准

![Straightforward Class Project: Apply NNets to Task](img/6f1cb40252f5729647d668b5b069d9ca.png)

*   7.**永远要接近你的数据**（除了最后的测试集）

    *   可视化数据集
    *   收集汇总统计信息
    *   查看错误
    *   分析不同的超参数如何影响性能

*   8.**通过良好的实验设置**，**尝试不同的模型和模型变体**，**达到快速迭代的目的**

    *   Fixed window neural model
    *   Recurrent neural network
    *   Recursive neural network
    *   Convolutional neural network
    *   Attention-basedmodel

# 7.结果呈现和评估

## 7.1 数据集

![Pots of data](img/7cbc447bb1e7f8e3f82208549a67daf1.png)

*   许多公开可用的数据集都是使用 train/dev/test 结构发布的。我们都在荣誉系统上，只在开发完成时才运行测试集

*   这样的分割假设有一个相当大的数据集

*   如果没有开发集或者你想要一个单独的调优集，那么你可以通过分割训练数据来创建一个调优集，尽管你必须权衡它的大小/有用性与训练集大小的减少

*   拥有一个固定的测试集，确保所有系统都使用相同的黄金数据进行评估。这通常是好的，但是如果测试集具有不寻常的属性，从而扭曲了任务的进度，那么就会出现问题。

## 7.2 训练模型与训练集

![Training models and pots of data](img/69a3576c3b1894955effa74a5c5c7cb9.png)

*   训练时,模型过拟合
    *   该模型正确地描述了你所训练的特定数据中发生的情况，但是模式还不够通用，不适合应用于新数据

*   监控和避免问题过度拟合的方法是使用独立的验证和测试集…

![Training models and pots of data](img/8409397d052fe399d6628c8bb466f472.png)

*   你在一个训练集上构建(评价/训练)一个模型。

*   通常，然后在另一个独立的数据集上设置进一步的超参数，即调优集
    *   调优集是用来调整超参数的训练集

*   在开发集(开发测试集或验证集)上度量进度
    *   如果你经常这样做，就会过度适应开发集，所以最好有第二个开发集，即 dev2set

*   只有最后,你评估和最终数据在一个测试集
    *   非常少地使用最终测试集……理想情况下只使用一次

![Training models and pots of data](img/cf21d8c5e86e9383accb737616c28133.png)

*   训练、调优、开发和测试集需要完全不同

*   在训练所使用的数据集上进行测试是无效的
    *   你将得到一个错误的良好性能。我们通常训练时会过拟合

*   你需要一个独立的调优
    *   如果调优与 train 相同，则无法正确设置超参数

*   如果你一直运行在相同的评价集，你开始在评价集上过拟合
    *   实际上，你是在对评估集进行“训练”……你在学习那些对特定的评估集有用和没用的东西，并利用这些信息

*   要获得系统性能的有效度量，你需要另一个未经训练的独立测试集，即 dev2 和最终测试

## 7.3 训练细节与建议

![Getting your neural network to train](img/0ee91279a41c50ffe68a9468a9760eae.png)

*   从积极的态度开始
    *   神经网络想要学习
    *   如果网络没有学习，你就是在做一些事情来阻止它成功地学习

*   认清残酷的现实
    *   有很多事情会导致神经网络完全不学习或者学习不好
    *   找到并修复它们(“调试和调优”)通常需要更多的时间，而不是实现你的模型

*   很难算出这些东西是什么
    *   但是经验、实验和经验法则会有所帮助！

## 7.4 关于学习率

![Models are sensitive to learning rates](img/a27632bb1a7527b848fcb7b3302fdbcb.png)

## 7.5 关于参数初始化

![Models are sensitive to initialization](img/516ff6ab68af1c6f57d890580c2540b5.png)

## 7.6 训练一个 RNN

![Training a (gated) RNN](img/11f5983f2ee003b8c215ae018e2b648d.png)

*   1.使用 LSTM 或 GRU：它使你的生活变得更加简单！

*   2.初始化递归矩阵为正交矩阵

*   3.用一个可感知的(小的)比例初始化其他矩阵

*   4.初始化忘记门偏差为 1：默认记住

*   5.使用自适应学习速率算法：Adam, AdaDelta，…

*   6.梯度范数的裁剪：1-5 似乎是一个合理的阈值，当与 Adam 或 AdaDelta 一起使用

*   7.要么只使用 dropout vertically，要么研究使用 Bayesian dropout(Gal 和 gahramani -不在 PyTorch 中原生支持)

*   8.要有耐心！优化需要时间

## 7.7 实验策略

![Experimental strategy](img/e4066d8f70660f8c3a4b17fbdeb2ad4f.png)

*   增量地工作！
*   从一个非常简单的模型开始
*   让它开始工作一个接一个地添加修饰物，让模型使用它们中的每一个(或者放弃它们)
*   最初运行在少量数据上

*   你会更容易在一个小的数据集中看到 bug

    *   像 8 个例子这样的东西很好
    *   通常合成数据对这很有用
    *   确保你能得到 100%的数据
    *   否则你的模型肯定要么不够强大，要么是破碎的

![Experimental strategy](img/92ede4182cda9885fdd444d3beea9d90.png)

*   在大型数据集中运行

    *   模型优化后的训练数据仍应接近 100%
        *   否则，你可能想要考虑一种更强大的模式来过拟合训练数据
        *   对训练数据的过拟合在进行深度学习时并不可怕
        *   这些模型通常善于一般化，因为分布式表示共享统计强度，和对训练数据的过度拟合无关

*   但是，现在仍然需要良好的泛化性能

    *   对模型进行正则化，直到它不与 dev 数据过拟合为止
        *   像 L2 正则化这样的策略是有用的
        *   但通常 Dropout 是成功的秘诀

## 7.8 模型细节

![Details matter!](img/d9cde7151187fd6dfe4f9e8c06dc5f09.png)

*   查看你的数据，收集汇总统计信息
*   查看你的模型的输出，进行错误分析
*   调优超参数对于神经网络几乎所有的成功都非常重要

## 7.9 项目总结

![Project writeup](img/a06640b644863e572ee6e8bc379f6238.png)

## 7.10 祝你项目顺利

![Good luck with your projects! ](img/343cded23f67a6b7ec56544606f7117f.png)

# 8.视频教程

可以点击 [B 站](https://www.bilibili.com/video/BV1Yo4y1D7FW?p=9) 查看视频的【双语字幕】版本

[`player.bilibili.com/player.html?aid=376755412&page=9`](https://player.bilibili.com/player.html?aid=376755412&page=9)

【双语字幕+资料下载】斯坦福 CS224n | 深度学习与自然语言处理(2019·全 20 讲)

# 9.参考资料

*   [本讲带学的**在线阅翻页本**](https://blog.showmeai.tech/cs224n/lecture09-Practical-Tips-for-Final-Projects#/)
*   [《斯坦福 CS224n 深度学习与自然语言处理》**课程学习指南**](https://blog.showmeai.tech/cs224n/)
*   [《斯坦福 CS224n 深度学习与自然语言处理》**课程大作业解析**](https://github.com/ShowMeAI-Hub/awesome-AI-courses-notes-cheatsheets/tree/main/CS224n-Natural-Language-Processing-with-Deep-Learning/assignment-solutions)
*   [【**双语字幕视频**】斯坦福 CS224n | 深度学习与自然语言处理(2019·全 20 讲)](https://www.bilibili.com/video/BV1Yo4y1D7FW)
*   [**Stanford 官网** | CS224n: Natural Language Processing with Deep Learning](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/)

# [**ShowMeAI**](http://www.showmeai.tech)系列教程推荐

*   [大厂技术实现 | 推荐与广告计算解决方案](http://www.showmeai.tech/tutorials/50)
*   [大厂技术实现 | 计算机视觉解决方案](http://www.showmeai.tech/tutorials/51)
*   [大厂技术实现 | 自然语言处理行业解决方案](http://www.showmeai.tech/tutorials/52)
*   [图解 Python 编程：从入门到精通系列教程](http://www.showmeai.tech/tutorials/56)
*   [图解数据分析：从入门到精通系列教程](http://www.showmeai.tech/tutorials/33)
*   [图解 AI 数学基础：从入门到精通系列教程](http://www.showmeai.tech/tutorials/83)
*   [图解大数据技术：从入门到精通系列教程](http://www.showmeai.tech/tutorials/84)
*   [图解机器学习算法：从入门到精通系列教程](http://www.showmeai.tech/tutorials/34)
*   [机器学习实战：手把手教你玩转机器学习系列](http://www.showmeai.tech/tutorials/41)
*   [深度学习教程 | 吴恩达专项课程 · 全套笔记解读](http://www.showmeai.tech/tutorials/35)
*   [自然语言处理教程 | 斯坦福 CS224n 课程 · 课程带学与全套笔记解读](http://www.showmeai.tech/tutorials/36)

# NLP 系列教程文章

*   [NLP 教程(1)- 词向量、SVD 分解与 Word2vec](http://showmeai.tech/article-detail/230)
*   [NLP 教程(2)- GloVe 及词向量的训练与评估](http://showmeai.tech/article-detail/232)
*   [NLP 教程(3)- 神经网络与反向传播](http://showmeai.tech/article-detail/234)
*   [NLP 教程(4)- 句法分析与依存解析](http://www.showmeai.tech/article-detail/237)
*   [NLP 教程(5)- 语言模型、RNN、GRU 与 LSTM](http://www.showmeai.tech/article-detail/239)
*   [NLP 教程(6)- 神经机器翻译、seq2seq 与注意力机制](http://www.showmeai.tech/article-detail/242)
*   [NLP 教程(7)- 问答系统](http://www.showmeai.tech/article-detail/245)
*   [NLP 教程(8)- NLP 中的卷积神经网络](http://www.showmeai.tech/article-detail/247)
*   [NLP 教程(9)- 句法分析与树形递归神经网络](http://www.showmeai.tech/article-detail/255)

# 斯坦福 CS224n 课程带学详解

*   [斯坦福 NLP 课程 | 第 1 讲 - NLP 介绍与词向量初步](http://showmeai.tech/article-detail/231)
*   [斯坦福 NLP 课程 | 第 2 讲 - 词向量进阶](http://showmeai.tech/article-detail/233)
*   [斯坦福 NLP 课程 | 第 3 讲 - 神经网络知识回顾](http://showmeai.tech/article-detail/235)
*   [斯坦福 NLP 课程 | 第 4 讲 - 神经网络反向传播与计算图](http://showmeai.tech/article-detail/236)
*   [斯坦福 NLP 课程 | 第 5 讲 - 句法分析与依存解析](http://www.showmeai.tech/article-detail/238)
*   [斯坦福 NLP 课程 | 第 6 讲 - 循环神经网络与语言模型](http://www.showmeai.tech/article-detail/240)
*   [斯坦福 NLP 课程 | 第 7 讲 - 梯度消失问题与 RNN 变种](http://www.showmeai.tech/article-detail/241)
*   [斯坦福 NLP 课程 | 第 8 讲 - 机器翻译、seq2seq 与注意力机制](http://www.showmeai.tech/article-detail/243)
*   [斯坦福 NLP 课程 | 第 9 讲 - cs224n 课程大项目实用技巧与经验](http://www.showmeai.tech/article-detail/244)
*   [斯坦福 NLP 课程 | 第 10 讲 - NLP 中的问答系统](http://www.showmeai.tech/article-detail/246)
*   [斯坦福 NLP 课程 | 第 11 讲 - NLP 中的卷积神经网络](http://www.showmeai.tech/article-detail/248)
*   [斯坦福 NLP 课程 | 第 12 讲 - 子词模型](http://www.showmeai.tech/article-detail/249)
*   [斯坦福 NLP 课程 | 第 13 讲 - 基于上下文的表征与 NLP 预训练模型](http://www.showmeai.tech/article-detail/250)
*   [斯坦福 NLP 课程 | 第 14 讲 - Transformers 自注意力与生成模型](http://www.showmeai.tech/article-detail/251)
*   [斯坦福 NLP 课程 | 第 15 讲 - NLP 文本生成任务](http://www.showmeai.tech/article-detail/252)
*   [斯坦福 NLP 课程 | 第 16 讲 - 指代消解问题与神经网络方法](http://www.showmeai.tech/article-detail/253)
*   [斯坦福 NLP 课程 | 第 17 讲 - 多任务学习(以问答系统为例)](http://www.showmeai.tech/article-detail/254)
*   [斯坦福 NLP 课程 | 第 18 讲 - 句法分析与树形递归神经网络](http://www.showmeai.tech/article-detail/256)
*   [斯坦福 NLP 课程 | 第 19 讲 - AI 安全偏见与公平](http://www.showmeai.tech/article-detail/257)
*   [斯坦福 NLP 课程 | 第 20 讲 - NLP 与深度学习的未来](http://www.showmeai.tech/article-detail/258)

![](img/542a6b9a82f30b506d0a5fef54ea8bd7.png)